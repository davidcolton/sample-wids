{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About the workshop","text":"<p>title: Granite Workshop description: Learn how to leverage the Granite Foundational Models logo: images/ibm-blue-background.png</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to our workshop! In this workshop we'll be using the open-sourced IBM Granite AI foundation models for a number of use cases that demonstrates the value of generative AI.</p> <p>By the end of this workshop, you will be able to:</p> <ul> <li>Summarize a text document using text summarization</li> <li>Generate specific information from a large document using the RAG technique</li> <li>Predict future trends using time series forecasting</li> <li>Generate programming code (Bash) by prompting a code model</li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>The introductory page of the workshop is broken down into the following sections:</p> <ul> <li>Introduction<ul> <li>About this workshop</li> </ul> </li> <li>Agenda</li> <li>Technology Used</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Link to Content Description Lab 0: Pre-work Pre-work for the workshop Lab 1: Document Summarization with Granite Learn how to use an AI model to summarize a work of literature Lab 2: Retrieval Augmented Generation (RAG) with Langchain Learn how to generate specific information from a large document Lab 3: Energy Demand Forecasting with Granite Timeseries (TTM) Learn how to predict future trends using time series forecasting Lab 4: Generating Bash Code with Granite Code Learn how to use an AI model to generate programming code"},{"location":"#technology-used","title":"Technology Used","text":"<p>The technology used in the workshop is as follows:</p> <ul> <li>IBM Granite AI foundation models](https://www.ibm.com/granite)</li> <li>Jupyter notebooks</li> <li>LangChain</li> <li>Ollama</li> <li>Replicate</li> </ul>"},{"location":"#credits","title":"Credits","text":"<p>The original authors and creators of this content were:</p> <ul> <li>BJ Hargrave</li> <li>Martin Hickey</li> <li>Ming Zhao</li> <li>The notebooks used in this workshop are versions of notebooks from the original Granite Workshop which are, in turn, versions from the IBM Granite Community modified for the workshop needs.</li> </ul>"},{"location":"lab-1/readme/","title":"Document Summarization with Granite","text":"<p>Text summarization condenses one or more texts into shorter summaries for enhanced information extraction.</p> <p>The goal of this lab is to show how you can use IBM Granite models in order to apply long document summarization techniques to a work of literature.</p>"},{"location":"lab-1/readme/#prerequisites","title":"Prerequisites","text":"<p>This lab is a Jupyter notebook. Please follow the instructions in pre-work to run the lab.</p>"},{"location":"lab-1/readme/#loading-the-lab","title":"Loading the Lab","text":"<p>To run the notebook from your command line in Jupyter using the active virtual environment from the pre-work, run:</p> <pre><code>jupyter-lab\n</code></pre> <p>When Jupyter Lab opens the path to the <code>notebooks/Summarize.ipynb</code> notebook file is relative to the <code>sample-wids</code> folder from the git clone in the pre-work. The folder navigation pane on the left-hand side can be used to navigate to the file. Once the notebook has been found it can be double clicked and it will open to the pane on the right. </p>"},{"location":"lab-1/readme/#running-and-lab-with-explanations","title":"Running and Lab (with explanations)","text":"<p>This notebook demonstrates an application of long document summarisation techniques to a work of literature using Granite.</p> <p>The notebook contains both <code>code</code> cells and <code>markdown</code> text cells. The text cells each give a brief overview of the code in the following code cell(s). These cells are not executable. You can execute the code cells by placing your cursor in the cell and then either hitting the Run this cell button at the top of the page or by pressing the <code>Shift</code> + <code>Enter</code> keys together. The main <code>code</code> cells are described in detail below.</p>"},{"location":"lab-1/readme/#selecting-your-model","title":"Selecting your model","text":"<p>Select a Granite model to use. Here we use a Langchain client to connect to  the model. If there is a locally accessible Ollama server, we use an  Ollama client to access the model. Otherwise, we use a Replicate client  to access the model.</p> <p>When using Replicate, if the <code>REPLICATE_API_TOKEN</code> environment variable is not set, or a <code>REPLICATE_API_TOKEN</code> Colab secret is not set, then the notebook will ask for your Replicate API token in a dialog box.</p> <pre><code>try:  # Look for a locally accessible Ollama server for the model\n    response = requests.get(os.getenv(\"OLLAMA_HOST\", \"http://127.0.0.1:11434\"))\n    model = OllamaLLM(\n        model=\"granite3.2:2b\",\n        num_ctx=65536,  # 64K context window\n    )\n    model = model.bind(raw=True)  # Client side controls prompt\nexcept Exception:  # Use Replicate for the model\n    model = Replicate(\n        model=\"ibm-granite/granite-3.2-8b-instruct\",\n        replicate_api_token=get_env_var(\"REPLICATE_API_TOKEN\"),\n        model_kwargs={\n            \"max_tokens\": 2000,  # Set the maximum number of tokens to generate as output.\n            \"min_tokens\": 200,  # Set the minimum number of tokens to generate as output.\n            \"temperature\": 0.75,\n            \"presence_penalty\": 0,\n            \"frequency_penalty\": 0,\n        },\n    )\n</code></pre> <p>In this first piece of code we try to determine if there is a local Ollama server running on <code>http://127.0.0.1:11434</code>. If the Ollama server is found then an <code>OllamaLLM</code> model instance is created for use later. If the Ollama server is not found the code then reverts to using the Granite 3.2-8b model served from Replicate .</p>"},{"location":"lab-1/readme/#chunk-document","title":"Chunk Document","text":"<pre><code>def chunk_document(\n    source: str,\n    *,\n    dropwhile: Callable[[BaseChunk], bool] = lambda c: False,\n    takewhile: Callable[[BaseChunk], bool] = lambda c: True,\n) -&gt; Iterator[BaseChunk]:\n    \"\"\"Read the document and perform a hierarchical chunking\"\"\"\n    converter = DocumentConverter()\n    chunks = HierarchicalChunker().chunk(converter.convert(source=source).document)\n    return itertools.takewhile(takewhile, itertools.dropwhile(dropwhile, chunks))\n</code></pre> <p>This Python function, <code>chunk_document</code>, is designed to perform hierarchical chunking on a given text document. Here's a breakdown of its components:</p> <ol> <li> <p>Function Signature: The function takes one required argument and two optional arguments. The required arguments are:</p> </li> <li> <p><code>source</code>: A string representing the text document to be chunked.</p> </li> <li><code>*</code>: Is a marker that all later arguments must be passed by keyword.</li> <li> <p>The optional arguments are:</p> <ul> <li><code>dropwhile</code> : A callable (function) that takes a <code>BaseChunk</code> object and returns a boolean. This function is used to determine when to stop dropping elements from the beginning of the chunks. The default is a lambda function that always returns <code>False</code>, meaning it will never drop any elements.</li> <li><code>takewhile</code>: A callable (function) that takes a <code>BaseChunk</code> object and returns a boolean. This function is used to determine when to stop taking elements from the beginning of the chunks. The default is a lambda function that always returns <code>True</code>, meaning it will take all elements.</li> </ul> </li> <li> <p>Document Conversion: The function first converts the input <code>source</code> string into a document object using a <code>DocumentConverter</code> instance (<code>converter</code> created on the first line of the function).</p> </li> <li> <p>Chunking: It then uses a <code>HierarchicalChunker</code> instance to perform hierarchical chunking on the document. The result is a list of <code>BaseChunk</code> objects.</p> </li> <li> <p>Itertools Dropwhile and Takewhile: Finally, the function uses <code>itertools.dropwhile</code> and <code>itertools.takewhile</code>to iterate over the chunks. The <code>dropwhile</code> function will drop elements from the beginning of the chunks as long as the <code>dropwhile</code> callable returns <code>True</code>. The <code>takewhile</code> </p> </li> </ol> <p>function will take elements from the remaining chunks as long as the <code>takewhile</code> callable returns <code>True</code>.</p> <ol> <li>Return Value: The function returns an iterator of <code>BaseChunk</code> objects, which represent the hierarchically chunked document.</li> </ol> <p>In summary, this function allows you to define custom conditions for dropping and taking chunks of a document, providing flexibility in how the document is segmented. The default behaviour is to not drop any chunks and to take all chunks.</p>"},{"location":"lab-1/readme/#merge-chunks","title":"Merge Chunks","text":"<pre><code>def merge_chunks(\n    chunks: Iterator[BaseChunk],\n    *,\n    headings: Callable[[BaseChunk], list[str]] = lambda c: c.meta.headings,\n) -&gt; Iterator[dict[str, str]]:\n    \"\"\"Merge chunks having the same headings\"\"\"\n    prior_headings: list[str] | None = None\n    document: dict[str, str] = {}\n    for chunk in chunks:\n        text = chunk.text.replace(\"\\r\\n\", \"\\n\")\n        current_headings = headings(chunk)\n        if prior_headings != current_headings:\n            if document:\n                yield document\n            prior_headings = current_headings\n            document = {\"title\": \" - \".join(current_headings), \"text\": text}\n        else:\n            document[\"text\"] += f\"\\n\\n{text}\"\n    if document:\n        yield document\n</code></pre> <ol> <li>Function Signature: The function is designed to merge chunks of text that share the same headings. It takes one required argument and one optional arguments. The required arguments are:</li> <li><code>chunks</code>: An an iterator of <code>BaseChunk</code> objects</li> <li><code>*</code>: Is a marker that all later arguments must be passed by keyword.</li> <li><code>headings</code>:An optional callable function that extracts headings from a chunk. The default function simply returns the <code>headings</code> attribute of the chunk's metadata.</li> <li>For each Chunk: The function iterates over each chunk in the input iterator. For each chunk, it extracts the text and headings. If the current chunk's headings differ from the previous chunk's headings, it yields the accumulated document (a dictionary with 'title' and 'text' keys) and starts a new document with the current chunk's headings and text. If the headings are the same, it appends the current chunk's text to the existing document.</li> <li>Finally: Finally, after the loop, if there's any remaining document (i.e., the last chunk in the iterator didn't end a section), it yields that document.</li> </ol> <p>The function returns an iterator of dictionaries, where each dictionary represents a merged chunk of text with its corresponding headings. The 'title' key in the dictionary is a concatenation of the headings with \" - \" as a separator, and the 'text' key contains the merged text of the chunks that share the same headings.</p>"},{"location":"lab-1/readme/#chunk-dropwhile","title":"Chunk Dropwhile","text":"<pre><code>def chunk_dropwhile(chunk: BaseChunk) -&gt; bool:\n    \"\"\"Ignore front matter prior to the book start\"\"\"\n    return \"WALDEN\" not in chunk.meta.headings\n</code></pre> <p>This Python function is designed to process chunks of data, specifically in the context of a document or book. The function takes one argument, <code>chunk</code>, which is expected to be an instance of a class or type named <code>BaseChunk</code>. This class or type is presumably defined elsewhere in the codebase and is likely used to represent a segment or part of a larger document.</p> <p>The <code>chunk</code> object has a property called <code>meta</code>, which is assumed to be an object containing metadata about the chunk. This metadata includes a list of headings, stored in <code>meta.headings</code>.</p> <p>The function checks if the string \"WALDEN\" is not in the list of headings. If \"WALDEN\" is not found in the headings, the function returns <code>True</code>, indicating that the chunk should be retained or processed further. If \"WALDEN\" is found in the headings, the function returns <code>False</code>, indicating that the chunk should be ignored or dropped.</p> <p>In essence, this function is used to filter out chunks that represent front matter (like a table of contents, preface, or introduction) before the main content of the book, which is assumed to start with the heading \"WALDEN\". This is a common pattern in text processing, where you want to skip over certain sections of a document.</p>"},{"location":"lab-1/readme/#chunk-takewhile","title":"Chunk Takewhile","text":"<pre><code>def chunk_takewhile(chunk: BaseChunk) -&gt; bool:\n    \"\"\"Ignore remaining chunks once we see this heading\"\"\"\n    return \"ON THE DUTY OF CIVIL DISOBEDIENCE\" not in chunk.meta.headings\n</code></pre> <p>This Python function, named <code>chunk_takewhile</code>, is designed to be used in a context where data is being processed in chunks. The function takes one argument, <code>chunk</code>, which is expected to be an instance of a class or subclass named <code>BaseChunk</code>.</p> <p>The purpose of this function is to determine whether to continue processing subsequent chunks or to stop processing based on the content of the current chunk. It does this by checking if a specific string, <code>\"ON THE DUTY OF CIVIL DISOBEDIENCE\"</code>, is present in the <code>headings</code> attribute of the <code>meta</code> attribute of the <code>chunk</code>  object.</p> <p>If the string is not found in the <code>headings</code>, the function returns <code>True</code>, indicating that the processing should continue with the next chunk. If the string is found, the function returns <code>False</code>, indicating that the processing should stop after the current chunk.</p> <p>In essence, this function acts as a filter or a condition for chunk processing, allowing you to control when to stop processing based on the content of the chunks. This can be particularly useful when dealing with large datasets or files that can be divided into smaller, more manageable chunks.</p>"},{"location":"lab-1/readme/#chunk-headings","title":"Chunk Headings","text":"<pre><code>def chunk_headings(chunk: BaseChunk) -&gt; list[str]:\n    \"\"\"Use the h1 and h2 (chapter) headings\"\"\"\n    return chunk.meta.headings[:2]\n</code></pre> <p>This Python function, named <code>chunk_headings</code>, is designed to extract the first two headings (h1 and h2) from a given chunk of content. The function takes one parameter, <code>chunk</code>, which is expected to be an instance of a class named <code>BaseChunk</code>.</p> <p>The <code>BaseChunk</code> class, which was imported from <code>docling_core.transforms.chunker.base</code> has a <code>meta</code> attribute, which itself is expected to be an object containing metadata about the chunk. This metadata object has a <code>headings</code> attribute, which is a list of strings representing the headings found in the chunk.</p> <p>The function returns a list containing the first two headings from this list. If there are fewer than two headings in the chunk, it will return all available headings.</p> <p>Here's a breakdown of the function:</p> <ul> <li>This function named <code>chunk_headings</code> takes one parameter,<code>chunk</code>, which is annotated to be an instance of <code>BaseChunk</code></li> <li>The function is expected to return a list of strings (<code>list[str]</code>).</li> <li><code>return chunk.meta.headings[:2]</code>. This line returns a slice of the <code>headings</code> list from the <code>meta</code> attribute of the <code>chunk</code> object. The slice <code>[:2]</code> indicates that only the first two elements of the list should be returned.</li> </ul> <p>In summary, this function is used to extract the first two headings (h1 and h2) from a chunk of content, which is used for creating a table of contents or for any other purpose that requires identifying the main sections of a document.</p>"},{"location":"lab-1/readme/#creating-the-document-objects-to-summarise","title":"Creating the Document Objects to Summarise","text":"<pre><code>documents: list[dict[str, str]] = list(\n    merge_chunks(\n        chunk_document(\n            \"https://www.gutenberg.org/cache/epub/205/pg205-images.html\",\n            dropwhile=chunk_dropwhile,\n            takewhile=chunk_takewhile,\n        ),\n        headings=chunk_headings,\n    )\n)\n</code></pre> <p>This Python code is fetching and processing a webpage from the Project Gutenberg website. Here's a breakdown:</p> <ol> <li><code>chunk_document</code>    This function is described in details earlier. It takes a URL and two other functions,<code>chunk_dropwhile</code> and <code>chunk_takewhile</code>, as arguments. It's responsible for fetching the webpage content and dividing it into smaller chunks based on the conditions defined by <code>chunk_dropwhile</code> and <code>chunk_takewhile</code>.</li> </ol> <p>might keep adding text to a chunk until a certain condition is no longer met. <code>chunk_headings</code> is another function that takes a chunk of text and returns a new chunk containing only the headings (HTML <code>&lt;h1&gt;</code> and <code>&lt;h2&gt;</code> tags) from the original chunk.</p> <ol> <li><code>merge_chunks</code>    Takes the chunks generated by <code>chunk_document</code> and combines them into a single list of dictionaries. Each dictionary represents a document with keys and values corresponding to the document's content and metadata, respectively.</li> </ol> <p>In summary, this code is fetching a webpage, dividing its content into chunks based on the defined conditions, extracting headings from each chunk, merging all chunks back into a single list, and storing the result as a list of dictionaries. Each dictionary in the list represents a document with its content and metadata.</p>"},{"location":"lab-1/readme/#credits","title":"Credits","text":"<p>This notebook is a modified version of the IBM Granite Community Document Summarization notebook. Refer to the IBM Granite Community for the official notebooks.</p>"},{"location":"pre-work/readme/","title":"Pre-work","text":"<p>The labs in the workshop are Jupyter notebooks. These notebooks are intended to be run locally on your computer. Follow these instructions to ensure that your laptop is ready to run the labs.</p>"},{"location":"pre-work/readme/#running-the-granite-notebooks-locally","title":"Running the Granite Notebooks Locally","text":"<p>It is recommended if you want to run the lab notebooks locally on your computer that you have:</p> <ul> <li>A computer or laptop</li> <li>Knowledge of Git and Python</li> </ul> <p>Running the lab notebooks locally on your computer requires the following steps:</p> <ul> <li>Local Prerequisites</li> <li>Clone the Granite Workshop Repository</li> <li>Serving the Granite AI Models</li> </ul>"},{"location":"pre-work/readme/#local-prerequisites","title":"Local Prerequisites","text":"<ul> <li>Git</li> <li>Uv</li> </ul>"},{"location":"pre-work/readme/#git","title":"Git","text":"<p>Git can be installed on the most common operating systems like Windows,  Mac, and Linux. In fact, Git comes installed by default on most Mac and  Linux machines!</p> <p>For comprehensive instructions on how to install <code>git</code> on your laptop please refer to the Install Git page.</p> <p>To confirm the you have <code>git</code> installed correctly you can open a terminal window and type <code>git version</code>. You should receive a response like the one shown below.</p> <pre><code>git version\ngit version 2.39.5 (Apple Git-154)\n</code></pre>"},{"location":"pre-work/readme/#uv","title":"Uv","text":"<p><code>uv</code> is an extremely fast Python package and project manager, written in Rust.</p> <p>For detailed instructions on how to install <code>uv</code> on your laptop please refer to the Installing uv page.</p> <p>To confirm the you have <code>uv</code> installed correctly you can open a terminal window and type <code>uv --version</code>. You should receive a response like the one shown below.</p> <pre><code>uv --version\nuv 0.6.12 (e4e03833f 2025-04-02)\n</code></pre>"},{"location":"pre-work/readme/#clone-the-granite-workshop-repository","title":"Clone the Granite Workshop Repository","text":"<p>Clone the workshop repo and cd into the repo directory.</p> <pre><code>git clone https://github.com/davidcolton/sample-wids.git\ncd sample-wids\n</code></pre>"},{"location":"pre-work/readme/#sync-the-python-virtual-environment","title":"Sync the Python Virtual Environment","text":"<p>The Sample WiDS repository uses a <code>pyproject.toml</code> file to define the version of Python to use and the required libraries to load. To sync your repository and setup Python and download your library dependancies run <code>uv sync</code> in a terminal. After syncing you have to activate your virtual environment.</p> <p>Note:</p> <p>If running on Windows it is suggested that you use the Windows Powershell running as administrator or, if you have it installed, the Windows Subsystem for Linux.</p> <pre><code>uv sync\n\n# Mac &amp; Linux\nsource .venv/bin/activatez\n\n# Windows Powershell\n.venv\\Scripts\\activate\n</code></pre>"},{"location":"pre-work/readme/#serving-the-granite-ai-models","title":"Serving the Granite AI Models","text":"<p>Lab 1: Document Summarization with Granite, Lab 2: Retrieval Augmented Generation (RAG) with Langchain and Lab 4: Generating Bash Code with Granite Code require Granite models to be served by an AI model runtime so that the models can be invoked or called. There are 2 options to serve the models as follows:</p> <ul> <li>Replicate AI Cloud Platform</li> <li>Running Ollama Locally OR</li> </ul>"},{"location":"pre-work/readme/#replicate-ai-cloud-platform","title":"Replicate AI Cloud Platform","text":"<p>Replicate is a cloud platform that will host and serve AI models for you.</p> <ol> <li> <p>Create a Replicate account. You will need a GitHub account to do this.</p> </li> <li> <p>Add credit to your Replicate Account (optional). To remove a barrier to entry to try the Granite models on the Replicate platform, use this link to add a small amount of credit to your Replicate account.</p> </li> <li> <p>Create a Replicate API Token.</p> </li> <li> <p>When you run the sample Notebooks you will be prompted to enter this token.</p> </li> <li> <p>Alternatively you can set your Replicate API Token as an environment variable in your terminal where you will run the notebook:</p> <pre><code>export REPLICATE_API_TOKEN=&lt;your_replicate_api_token&gt;\n</code></pre> </li> </ol>"},{"location":"pre-work/readme/#running-ollama-locally","title":"Running Ollama Locally","text":"<p>If you want to run the AI models locally on your computer, you can use Ollama. You will need to have a computer with:</p> <ul> <li>GPU processor</li> <li>At least 32GB RAM</li> </ul> <p>Tested system</p> <p>This was tested on a Macbook with an M1 processor and 32GB RAM. It maybe possible to serve models with a CPU and less memory.</p> <p>If you computer is unable to serve the models, then it is recommended to go to the Replicate AI Cloud Platform section instead.</p> <p>Running Ollama locally on your computer requires the following steps:</p> <ol> <li> <p>Download and install Ollama, if you haven't already. Ollama v0.3.14+ is required, so please upgrade if on an earlier version.</p> <p>On macOS, you can use Homebrew to install with</p> <pre><code>brew install ollama\n</code></pre> </li> <li> <p>Start the Ollama server. You will leave this running during the workshop.</p> <pre><code>ollama serve\n</code></pre> </li> <li> <p>In another terminal window, pull down the Granite models you will want to use in the workshop. Larger models take more memory to run but can give better results.</p> <pre><code>ollama pull granite3.2:2b\nollama pull granite3.2:8b\n</code></pre> <p>For Lab 4: Generating Bash Code with Granite Code, you will also need at least one of the following Granite Code models.</p> <pre><code>ollama pull granite-code:3b\nollama pull granite-code:8b\n</code></pre> </li> </ol>"}]}