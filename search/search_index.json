{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About the workshop","text":"<p>title: Granite Workshop description: Learn how to leverage the Granite Foundational Models logo: images/ibm-blue-background.png</p>"},{"location":"#introduction","title":"Introduction","text":"<p>Welcome to our workshop! In this workshop we'll be using the open-sourced IBM Granite AI foundation models for a number of use cases that demonstrates the value of generative AI.</p> <p>By the end of this workshop, you will be able to:</p> <ul> <li>Summarize a text document using text summarization</li> <li>Generate specific information from a large document using the RAG technique</li> <li>Predict future trends using time series forecasting</li> <li>Generate programming code (Bash) by prompting a code model</li> </ul>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>The introductory page of the workshop is broken down into the following sections:</p> <ul> <li>Introduction<ul> <li>About this workshop</li> </ul> </li> <li>Agenda</li> <li>Technology Used</li> <li>Credits</li> </ul>"},{"location":"#agenda","title":"Agenda","text":"Link to Content Description Lab 0: Pre-work Pre-work for the workshop Lab 1: Document Summarization with Granite Learn how to use an AI model to summarize a work of literature Lab 2: Retrieval Augmented Generation (RAG) with Langchain Learn how to generate specific information from a large document Lab 3: Energy Demand Forecasting with Granite Timeseries (TTM) Learn how to predict future trends using time series forecasting Lab 4: Generating Bash Code with Granite Code Learn how to use an AI model to generate programming code"},{"location":"#technology-used","title":"Technology Used","text":"<p>The technology used in the workshop is as follows:</p> <ul> <li>IBM Granite AI foundation models](https://www.ibm.com/granite)</li> <li>Jupyter notebooks</li> <li>LangChain</li> <li>Ollama</li> <li>Replicate</li> </ul>"},{"location":"#credits","title":"Credits","text":"<p>The original authors and creators of this content were:</p> <ul> <li>BJ Hargrave</li> <li>Martin Hickey</li> <li>Ming Zhao</li> <li>The notebooks used in this workshop are versions of notebooks from the original Granite Workshop which are, in turn, versions from the IBM Granite Community modified for the workshop needs.</li> </ul>"},{"location":"lab-1/readme/","title":"Document Summarization with Granite","text":"<p>Text summarization condenses one or more texts into shorter summaries for enhanced information extraction.</p> <p>The goal of this lab is to show how you can use IBM Granite models in order to apply long document summarization techniques to a work of literature.</p>"},{"location":"lab-1/readme/#prerequisites","title":"Prerequisites","text":"<p>This lab is a Jupyter notebook. Please follow the instructions in pre-work to run the lab.</p>"},{"location":"lab-1/readme/#lab","title":"Lab","text":"<p>To run the notebook from your command line in Jupyter using the active virtual environment from the pre-work, run:</p> <pre><code>jupyter-lab\n</code></pre> <p>When Jupyter Lab opens the path to the <code>notebooks/Summarize.ipynb</code> notebook file is relative to the <code>sample-wids</code> folder from the git clone in the pre-work. The folder navigation pane on the left-hand side can be used to navigate to the file. Once the notebook has been found it can be double clicked and it will open to the pane on the right. </p>"},{"location":"lab-1/readme/#credits","title":"Credits","text":"<p>This notebook is a modified version of the IBM Granite Community Document Summarization notebook. Refer to the IBM Granite Community for the official notebooks.</p>"},{"location":"pre-work/readme/","title":"Pre-work","text":"<p>The labs in the workshop are Jupyter notebooks. These notebooks are intended to be run locally on your computer. Follow these instructions to ensure that your laptop is ready to run the labs.</p>"},{"location":"pre-work/readme/#running-the-granite-notebooks-locally","title":"Running the Granite Notebooks Locally","text":"<p>It is recommended if you want to run the lab notebooks locally on your computer that you have:</p> <ul> <li>A computer or laptop</li> <li>Knowledge of Git and Python</li> </ul> <p>Running the lab notebooks locally on your computer requires the following steps:</p> <ul> <li>Local Prerequisites</li> <li>Clone the Granite Workshop Repository</li> <li>Serving the Granite AI Models</li> </ul>"},{"location":"pre-work/readme/#local-prerequisites","title":"Local Prerequisites","text":"<ul> <li>Git</li> <li>Uv</li> </ul>"},{"location":"pre-work/readme/#git","title":"Git","text":"<p>Git can be installed on the most common operating systems like Windows,  Mac, and Linux. In fact, Git comes installed by default on most Mac and  Linux machines!</p> <p>For comprehensive instructions on how to install <code>git</code> on your laptop please refer to the Install Git page.</p> <p>To confirm the you have <code>git</code> installed correctly you can open a terminal window and type <code>git version</code>. You should receive a response like the one shown below.</p> <pre><code>$ git version\ngit version 2.39.5 (Apple Git-154)\n</code></pre>"},{"location":"pre-work/readme/#uv","title":"Uv","text":"<p><code>uv</code> is an extremely fast Python package and project manager, written in Rust.</p> <p>For detailed instructions on how to install <code>uv</code> on your laptop please refer to the Installing uv page.</p> <p>To confirm the you have <code>uv</code> installed correctly you can open a terminal window and type <code>uv --version</code>. You should receive a response like the one shown below.</p> <pre><code>$ uv --version\nuv 0.6.12 (e4e03833f 2025-04-02)\n</code></pre>"},{"location":"pre-work/readme/#clone-the-granite-workshop-repository","title":"Clone the Granite Workshop Repository","text":"<p>Clone the workshop repo and cd into the repo directory.</p> <pre><code>$ git clone https://github.com/davidcolton/sample-wids.git\n$ cd sample-wids\n</code></pre>"},{"location":"pre-work/readme/#sync-the-python-virtual-environment","title":"Sync the Python Virtual Environment","text":"<p>The Sample WiDS repository uses a <code>pyproject.toml</code> file to define the version of Python to use and the required libraries to load. To sync your repository and setup Python and download your library dependancies run <code>uv sync</code> in a terminal. After syncing you have to activate your virtual environment.</p> <pre><code>$ uv sync\n$ source .venv/bin/activate\n</code></pre>"},{"location":"pre-work/readme/#serving-the-granite-ai-models","title":"Serving the Granite AI Models","text":"<p>Lab 1: Document Summarization with Granite, Lab 2: Retrieval Augmented Generation (RAG) with Langchain and Lab 4: Generating Bash Code with Granite Code require Granite models to be served by an AI model runtime so that the models can be invoked or called. There are 2 options to serve the models as follows:</p> <ul> <li>Replicate AI Cloud Platform</li> <li>Running Ollama Locally OR</li> </ul>"},{"location":"pre-work/readme/#replicate-ai-cloud-platform","title":"Replicate AI Cloud Platform","text":"<p>Replicate is a cloud platform that will host and serve AI models for you.</p> <ol> <li> <p>Create a Replicate account. You will need a GitHub account to do this.</p> </li> <li> <p>Add credit to your Replicate Account (optional). To remove a barrier to entry to try the Granite models on the Replicate platform, use this link to add a small amount of credit to your Replicate account.</p> </li> <li> <p>Create a Replicate API Token.</p> </li> <li> <p>Set your Replicate API Token as an environment variable in your terminal where you will run the notebook:</p> <pre><code>export REPLICATE_API_TOKEN=&lt;your_replicate_api_token&gt;\n</code></pre> </li> </ol>"},{"location":"pre-work/readme/#running-ollama-locally","title":"Running Ollama Locally","text":"<p>If you want to run the AI models locally on your computer, you can use Ollama. You will need to have a computer with:</p> <ul> <li>GPU processor</li> <li>At least 32GB RAM</li> </ul> <p>Tested system</p> <p>This was tested on a Macbook with an M1 processor and 32GB RAM. It maybe possible to serve models with a CPU and less memory.</p> <p>If you computer is unable to serve the models, then it is recommended to go to the Replicate AI Cloud Platform section instead.</p> <p>Running Ollama locally on your computer requires the following steps:</p> <ol> <li> <p>Download and install Ollama, if you haven't already. Ollama v0.3.14+ is required, so please upgrade if on an earlier version.</p> <p>On macOS, you can use Homebrew to install with</p> <pre><code>brew install ollama\n</code></pre> </li> <li> <p>Start the Ollama server. You will leave this running during the workshop.</p> <pre><code>ollama serve\n</code></pre> </li> <li> <p>In another terminal window, pull down the Granite models you will want to use in the workshop. Larger models take more memory to run but can give better results.</p> <pre><code>ollama pull granite3.2:2b\nollama pull granite3.2:8b\n</code></pre> <p>For Lab 4: Generating Bash Code with Granite Code, you will also need at least one of the following Granite Code models.</p> <pre><code>ollama pull granite-code:3b\nollama pull granite-code:8b\n</code></pre> </li> </ol>"}]}